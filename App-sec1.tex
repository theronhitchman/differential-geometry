\documentclass[Shifrin_Solutions_Spring_2015.tex]{subfiles}
\begin{document}


\section{Linear Algebra Review}

\begin{exercise} Suppose that $\{ v_1, v_2 \}$ is a basis for $\mathbb{R}^2$. Given vectors $x, y \in \mathbb{R}^2$, prove that $x=y$ if and only if $x \cdot v_i = y \cdot v_i$ for $i = 1, 2$.
\end{exercise}

\begin{proof}[Solution] Since $\{v_1, v_2 \}$ is a basis, we may write $x$ and $y$ in the form
\[
x = (x\cdot v_1) v_1 + (x\cdot v_2) v_2 , \qquad y = (y\cdot v_1) v_1 + (y\cdot v_2) v_2 .
\]
So clearly the condition in the exercise implies that $x = y$.

On the other hand, if $x=y$, then we may compute that
\[
0 = x-y = (  x\cdot v_1 - y \cdot v_1) v_1 + (x\cdot v_2 - y\cdot v_2) v_2 .
\]
But the expressions for vectors as linear combinations of $v_1$ and $v_2$ are unique. Thus each of the coefficients in the last equation must be zero, which implies the condition in the exercise.
\end{proof}

\vspace{1cm}

%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{exercise} The geometric-arithmetic mean inequality states that
\[
\sqrt{ab\,} \leq \dfrac{a+b}{2}, \text{ for positive numbers $a$ and $b$, }
\]
with equality holding if and only if $a=b$.
Give a one-line proof using the Cauchy-Schwarz inequality.
\end{exercise}

\begin{proof}[Solution]
Consider the vectors $ u = \begin{pmatrix} \sqrt{a }\\ \sqrt{b} \end{pmatrix}$ and $v = \begin{pmatrix} \sqrt{b } \\ \sqrt{a } \end{pmatrix}$. Then we compute directly that $u \cdot v = 2 \sqrt{ab }$
and $|| u || = ||v|| = \sqrt{ a+ b}$. The Cauchy-Schwarz inequality now reads that $2 \sqrt{ab} \leq \sqrt{a+b} \sqrt{a+b} = a + b$, which is the desired inequality.

Now, equality holds in Cauchy-Schwarz if and only if $u$ and $v$ are scalar multiples. This means that there is a scalar $\lambda$ such that $\sqrt{a} = \lambda\sqrt{b}$ and $\sqrt{b}= \lambda\sqrt{a}$. So, if equality holds, we conclude that there is a scalar $\lambda$ such that $\sqrt{a} = \lambda^2 \sqrt{a}$. Since $\lambda$ must be positive in this situation, we deduce that $\lambda = 1$ and hence that $a = b$. It is clear that if $a=b$ then equality holds in the geometric-arithmetic mean inequality.
\end{proof}

\vspace{1cm}

%%%%%%%%%%%%%%%%%%%%%


\begin{exercise} Let $w, x, y, z \in \mathbb{R}^3$. Prove that
\[
(w \times x) \cdot (y \times z) = (w\cdot y)(x \cdot z) - (w\cdot z)(x\cdot y).
\]
\end{exercise}

\begin{proof}[Solution] Shifrin's hint is about a common proof technique from more advanced differential geometry. (Well, it is really a linear algebra trick, but it is very commonly used in differential geometry.) The idea is this: if $S, T$ are two linear mappings, then they are equal if and only if they agree for each vector in a basis. For this exercise, both the right-hand and left-hand sides are linear functions of $w, x, y$ and $z$, so it really suffices to check that equality holds as long as each of the vectors comes from the standard basis $\{e_1, e_2, e_3 \}$ of $\mathbb{R}^3$. This simplifies things a bit, but leave a really long list of things to check.

Here is a more direct route, which is computational in nature. Write each of the vectors as a linear combination of the standard basis, and use these expressions to compute the two sides of the required equation. After some tedious computation, it becomes clear that both sides are equal to
\[
(w_2 x_3 - w_3 x_2) ( y_2 z_3 - y_3z_2 ) + (x_3 w_1 - x_1 w_2) (z_3 y_1 - z_1 y_3) + (w_1x_2 - w_2 x_1) (y_1z_2 - y_2 z_1) .
\]
This is pretty quick for the left-hand side, but to get the right-hand side into this form takes just a little rearranging.
\end{proof}


\vspace{1cm}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{exercise} Suppose that $A(t)$ is a differentiable family of $3\times 3$ orthogonal matrices. Prove that ${A(t)}^{-1}A'(t)$ is always skew-symmetric.
\end{exercise}

\begin{proof}[Solution]
Since each matrix $A(t)$ is orthogonal, we know that $ {A(t)}^T A(t)= I$, where $I$ is the $3\times 3$ identity matrix. We differentiate this equation to find
\[
 {A'(t)}^T A(t)  + {A(t)}^T A'(t) = 0 .
\]
We use this and the fact that for an orthogonal matrix $A^T = A^{-1}$ to see that
\[
[{A(t)}^{-1} A'(t) ]^T = [ {A(t)}^T A'(t)]^T = [- {A'(t)}^T A(t) ]^T = - {A(t)}^T A'(t) = - {A(t)}^{-1} A'(t) .
\]
This is exactly the condition for $A^{-1}A'$ to be skew-symmetric.
\end{proof}


\vspace{1cm}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{exercise} If $A = \begin{bmatrix} a & b \\ b & c \end{bmatrix}$ is a symmetric $2 \times 2$ matrix, set $f(x) = (Ax) \cdot x$ and check that $\nabla f (x) = 2 A x$.
 \end{exercise}

\begin{proof}[Solution] This is a spot where Shifrin is a bit sloppy about row versus column vectors, so be careful. Here all the vectors should be column vectors. We write $x = \begin{pmatrix} x_1 \\ x_2 \end{pmatrix}$.\\
We can compute that
\[
f(x) =\left( A \begin{pmatrix} x_1 \\ x_2 \end{pmatrix}\right) \cdot \begin{pmatrix} x_1 \\ x_2 \end{pmatrix}= \begin{pmatrix} ax_1+bx_2 \\ bx_1 + cx_2 \end{pmatrix} \cdot \begin{pmatrix} x_1 \\ x_2 \end{pmatrix} =  a x_1^2 + 2b x_1x_2 + cx_2^2
\]
 Then it follows quickly that
\[
\nabla f = \begin{pmatrix} 2ax_1 + 2bx_2 \\ 2bx_1 + 2cx_2 \end{pmatrix} = 2 Ax .
\]
\end{proof}

\end{document}